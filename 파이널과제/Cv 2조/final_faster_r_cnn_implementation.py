# -*- coding: utf-8 -*-
"""Final_Faster R-CNN_implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aFqapjofj5w_CFayQ_9CP6n0c0UOkrlu

## 1. 초기 패키지 선언
"""

import tensorflow as tf
import tensorflow_hub as hub


import matplotlib.pyplot as plt 
import tempfile 
from six.moves.urllib.request import urlopen 
from six import BytesIO 


import numpy as np


from PIL import Image 
from PIL import ImageColor 
from PIL import ImageDraw 
from PIL import ImageFont 
from PIL import ImageOps 

import time 


print("Tensorflow version:", tf.__version__)
print("The following GPU devices are available: %s" % tf.test.gpu_device_name())

"""## 2. 이미지 resizing, bounding box 계산 및 생성 등 이미지 작업"""

def download_and_resize_image(url, new_width=256, new_height=256,
                              display=False):
  _, filename = tempfile.mkstemp(suffix=".jpg")
  response = urlopen(url)
  image_data = response.read() 
  image_data = BytesIO(image_data) 
  pil_image = Image.open(image_data) 
  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS) 
  pil_image_rgb = pil_image.convert("RGB") 
  pil_image_rgb.save(filename, format="JPEG", quality=90) 
  print("Image downloaded to %s." % filename)
  if display:
    display_image(pil_image)
  return filename


def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, color,font,thickness=4,display_str_list=()):

   
  draw = ImageDraw.Draw(image)
  im_width, im_height = image.size 
  (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)
  
  
  draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)],
            width=thickness, fill=color)   
  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list] 
  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights) 

 
  if top > total_display_str_height: text_bottom = top
  else: text_bottom = top + total_display_str_height


  for display_str in display_str_list[::-1]:
    text_width, text_height = font.getsize(display_str)
    margin = np.ceil(0.05 * text_height)
    draw.rectangle([(left, text_bottom - text_height - 2 * margin),
                    (left + text_width, text_bottom)],
                   fill=color)
    draw.text((left + margin, text_bottom - text_height - margin),
              display_str, fill="black", font=font)
    text_bottom -= text_height - 2 * margin


def draw_boxes(image, boxes, class_names, scores, max_boxes=50, min_score=0.1):
  
  colors = list(ImageColor.colormap.values())
  font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf",20)
  
  for i in range(min(boxes.shape[0], max_boxes)):
    if scores[i] >= min_score:
      ymin, xmin, ymax, xmax = tuple(boxes[i])  
      display_str = "{}: {}%".format(class_names[i].decode("ascii"), int(100 * scores[i]))
      color = colors[hash(class_names[i]) % len(colors)]
      image_pil = Image.fromarray(np.uint8(image)).convert("RGB")
      draw_bounding_box_on_image(image_pil, ymin, xmin, ymax, xmax, color, font, display_str_list=[display_str])
      np.copyto(image, np.array(image_pil))
  return image


def display_image(image):
  fig = plt.figure(figsize=(20, 15))
  plt.grid(False)
  plt.imshow(image)

"""## 3. 예시 이미지 적용"""

image_url = "https://www.hanyang.ac.kr/documents/20182/5404223/02153445495.jpg" 
downloaded_image_path = download_and_resize_image(image_url, 800, 600, True)

"""## 4. Google Hub에서 ML 모듈 가져오기"""

module_handle1 = "https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1"

detector_faster_Rcnn = hub.load(module_handle1).signatures['default']

"""## 5. 적용할 이미지를 tensor로 변환 및 detection 실행

"""

def load_img(path):
  img = tf.io.read_file(path)    
  img = tf.image.decode_jpeg(img, channels=3)  
  return img

def run_detector(detector, path):
  img = load_img(path) 
  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...] 
  result = detector(converted_img) 

  
  start_time = time.time()  
  result = {key:value.numpy() for key,value in result.items()}
  end_time = time.time() 

  print("이미지 추론 갯수(Found %d objects)" %len(result["detection_scores"]))
  print("추론 시간(Inference time): ", end_time-start_time)
  
  
  image_with_boxes = draw_boxes(img.numpy(), result["detection_boxes"], result["detection_class_entities"], result["detection_scores"])
  display_image(image_with_boxes)

"""## 6. Faster R-CNN 적용"""

run_detector(detector_faster_Rcnn, downloaded_image_path)

"""## 7. 다양한 이미지에 Faster R-CNN 적용"""

image_urls = [
  "https://www.yonsei.ac.kr/_attach/image/sc/2020/11/thumb_68da63674da9d69f555a255a71f81e64.jpg",
  "https://www.yonsei.ac.kr/_attach/image/sc/2020/11/thumb_8b5bd57971999961c46b54f1fb981ed8.jpg",
  "https://www.yonsei.ac.kr/_attach/image/sc/2020/11/thumb_4d0e32b499f204a3489bb9245650c7fe.jpg",
  "https://www.digidirect.com.au/media/blogs/19_08_AUG/Wide_angle_lens_landscape/pietro-de-grandi-T7K4aEPoGGk-unsplash.jpg",
  "https://www.digidirect.com.au/media/blogs/19_08_AUG/Wide_angle_lens_landscape/michal-kmet-M9O6GRrEEDY-unsplash.jpg",
  "http://www.bloter.net/wp-content/uploads/2016/08/13239928_1604199256575494_4289308691415234194_n.jpg",
  "http://www.bloter.net/wp-content/uploads/2016/08/%EC%8A%A4%EB%A7%88%ED%8A%B8%ED%8F%B0-%EC%82%AC%EC%A7%84.jpg",
  "http://www.thelec.kr/news/photo/201911/4163_3843_2749.jpg",
  "https://www.yonsei.ac.kr/_attach/editor_image/2020-11/jqfmciyowklk.jpg",
  "https://www.yonsei.ac.kr/_attach/editor_image/2020-11/baekyvbfwawg.jpg"
   
  ]


def detect_img(image_url): 
  start_time = time.time() 
  image_path = download_and_resize_image(image_url, 640, 480) 
  run_detector(detector_faster_Rcnn, image_path) 
  end_time = time.time()

for i in range (10):
  detect_img(image_urls[i])

