{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from modules import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 1024\n",
    "CNN_embed_dim = 256     # latent dim extracted by 2D CNN\n",
    "res_size = 224        # ResNet image size\n",
    "dropout_p = 0.2       # dropout probability\n",
    "\n",
    "# training parameters\n",
    "epochs = 20        # training epochs\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "log_interval = 10   # interval for displaying training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_model_path = './results_MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mkdir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    MSE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X_reconst, z, mu, logvar = model(X)  # VAE\n",
    "        loss = loss_function(X_reconst, X, mu, logvar)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_y.extend(y.data.cpu().numpy())\n",
    "        all_z.extend(z.data.cpu().numpy())\n",
    "        all_mu.extend(mu.data.cpu().numpy())\n",
    "        all_logvar.extend(logvar.data.cpu().numpy())\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "\n",
    "    all_y = np.stack(all_y, axis=0)\n",
    "    all_z = np.stack(all_z, axis=0)\n",
    "    all_mu = np.stack(all_mu, axis=0)\n",
    "    all_logvar = np.stack(all_logvar, axis=0)\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(model.state_dict(), os.path.join(save_model_path, 'model_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y, all_z, all_mu, all_logvar = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "            X_reconst, z, mu, logvar = model(X)\n",
    "\n",
    "            loss = loss_function(X_reconst, X, mu, logvar)\n",
    "            test_loss += loss.item()  # sum up batch loss\n",
    "\n",
    "            all_y.extend(y.data.cpu().numpy())\n",
    "            all_z.extend(z.data.cpu().numpy())\n",
    "            all_mu.extend(mu.data.cpu().numpy())\n",
    "            all_logvar.extend(logvar.data.cpu().numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    all_y = np.stack(all_y, axis=0)\n",
    "    all_z = np.stack(all_z, axis=0)\n",
    "    all_mu = np.stack(all_mu, axis=0)\n",
    "    all_logvar = np.stack(all_logvar, axis=0)\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}\\n'.format(len(test_loader.dataset), test_loss))\n",
    "    return X.data.cpu().numpy(), all_y, all_z, all_mu, all_logvar, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab0f76f099446e8998f93ecdf58a4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f38825e5362440793ba7bd013ed33a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c4ae3a26ac40c8a39575f09d01e60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60221cb8a52b48bbb39d3eddf99f0ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Data loading parameters\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "transform = transforms.Compose([transforms.Resize([res_size, res_size]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # gray -> GRB 3 channel (lambda function)\n",
    "                                transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])])  # for grayscale images\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "MNIST_train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "MNIST_test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=MNIST_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=MNIST_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPU!\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "resnet_vae = ResNet_VAE(fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2, drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPU!\")\n",
    "model_params = list(resnet_vae.parameters())\n",
    "optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "check_mkdir(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [500/60000 (1%)]\tLoss: 5037605.000000\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 4767686.000000\n",
      "Train Epoch: 1 [1500/60000 (2%)]\tLoss: 4573765.000000\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 4445742.500000\n",
      "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 4297249.500000\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 4239515.500000\n",
      "Train Epoch: 1 [3500/60000 (6%)]\tLoss: 4147637.250000\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 4081292.500000\n",
      "Train Epoch: 1 [4500/60000 (8%)]\tLoss: 4058151.750000\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 3971434.250000\n",
      "Train Epoch: 1 [5500/60000 (9%)]\tLoss: 4008540.500000\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 3935330.000000\n",
      "Train Epoch: 1 [6500/60000 (11%)]\tLoss: 3865635.500000\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 3847348.500000\n",
      "Train Epoch: 1 [7500/60000 (12%)]\tLoss: 3817367.750000\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 3727862.750000\n",
      "Train Epoch: 1 [8500/60000 (14%)]\tLoss: 3797844.500000\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 3673710.500000\n",
      "Train Epoch: 1 [9500/60000 (16%)]\tLoss: 3702797.250000\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 3788649.750000\n",
      "Train Epoch: 1 [10500/60000 (18%)]\tLoss: 3597574.500000\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 3583690.750000\n",
      "Train Epoch: 1 [11500/60000 (19%)]\tLoss: 3534363.750000\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 3510039.500000\n",
      "Train Epoch: 1 [12500/60000 (21%)]\tLoss: 3448665.000000\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 3499180.750000\n",
      "Train Epoch: 1 [13500/60000 (22%)]\tLoss: 3450433.750000\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 3341706.500000\n",
      "Train Epoch: 1 [14500/60000 (24%)]\tLoss: 3401609.250000\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 3405303.750000\n",
      "Train Epoch: 1 [15500/60000 (26%)]\tLoss: 3237714.750000\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 3267739.750000\n",
      "Train Epoch: 1 [16500/60000 (28%)]\tLoss: 3244161.250000\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 3207319.500000\n",
      "Train Epoch: 1 [17500/60000 (29%)]\tLoss: 3199961.000000\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 3127864.750000\n",
      "Train Epoch: 1 [18500/60000 (31%)]\tLoss: 3148865.250000\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 3205190.000000\n",
      "Train Epoch: 1 [19500/60000 (32%)]\tLoss: 3100860.750000\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 3149231.750000\n",
      "Train Epoch: 1 [20500/60000 (34%)]\tLoss: 3058364.750000\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 3052868.000000\n",
      "Train Epoch: 1 [21500/60000 (36%)]\tLoss: 3074498.000000\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 3073584.750000\n",
      "Train Epoch: 1 [22500/60000 (38%)]\tLoss: 3009131.250000\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 2994911.500000\n",
      "Train Epoch: 1 [23500/60000 (39%)]\tLoss: 2968760.750000\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 2924182.000000\n",
      "Train Epoch: 1 [24500/60000 (41%)]\tLoss: 2869904.250000\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 2897284.000000\n",
      "Train Epoch: 1 [25500/60000 (42%)]\tLoss: 2882556.250000\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 2829309.750000\n",
      "Train Epoch: 1 [26500/60000 (44%)]\tLoss: 2886259.000000\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 2841230.250000\n",
      "Train Epoch: 1 [27500/60000 (46%)]\tLoss: 2816291.250000\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 2781474.500000\n",
      "Train Epoch: 1 [28500/60000 (48%)]\tLoss: 2758946.250000\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 2789552.750000\n",
      "Train Epoch: 1 [29500/60000 (49%)]\tLoss: 2782621.500000\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2655333.250000\n",
      "Train Epoch: 1 [30500/60000 (51%)]\tLoss: 2672316.750000\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 2616625.750000\n",
      "Train Epoch: 1 [31500/60000 (52%)]\tLoss: 2603585.000000\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2592986.500000\n",
      "Train Epoch: 1 [32500/60000 (54%)]\tLoss: 2604359.000000\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 2603837.500000\n",
      "Train Epoch: 1 [33500/60000 (56%)]\tLoss: 2542514.500000\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 2558468.750000\n",
      "Train Epoch: 1 [34500/60000 (58%)]\tLoss: 2501787.000000\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 2582286.500000\n",
      "Train Epoch: 1 [35500/60000 (59%)]\tLoss: 2511983.000000\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 2506832.750000\n",
      "Train Epoch: 1 [36500/60000 (61%)]\tLoss: 2444673.500000\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 2454494.250000\n",
      "Train Epoch: 1 [37500/60000 (62%)]\tLoss: 2440757.250000\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 2407716.500000\n",
      "Train Epoch: 1 [38500/60000 (64%)]\tLoss: 2459437.000000\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 2367989.000000\n",
      "Train Epoch: 1 [39500/60000 (66%)]\tLoss: 2402960.500000\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2395261.750000\n",
      "Train Epoch: 1 [40500/60000 (68%)]\tLoss: 2339471.000000\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 2425315.250000\n",
      "Train Epoch: 1 [41500/60000 (69%)]\tLoss: 2328444.000000\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 2339796.750000\n",
      "Train Epoch: 1 [42500/60000 (71%)]\tLoss: 2382335.250000\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 2250888.750000\n",
      "Train Epoch: 1 [43500/60000 (72%)]\tLoss: 2228416.000000\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 2237479.250000\n",
      "Train Epoch: 1 [44500/60000 (74%)]\tLoss: 2254925.500000\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 2245556.750000\n",
      "Train Epoch: 1 [45500/60000 (76%)]\tLoss: 2252172.500000\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 2279052.000000\n",
      "Train Epoch: 1 [46500/60000 (78%)]\tLoss: 2182308.500000\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 2198590.250000\n",
      "Train Epoch: 1 [47500/60000 (79%)]\tLoss: 2269848.750000\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2231194.000000\n",
      "Train Epoch: 1 [48500/60000 (81%)]\tLoss: 2152731.000000\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 2130641.750000\n",
      "Train Epoch: 1 [49500/60000 (82%)]\tLoss: 2080137.250000\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2184686.750000\n",
      "Train Epoch: 1 [50500/60000 (84%)]\tLoss: 2182319.000000\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 2092302.625000\n",
      "Train Epoch: 1 [51500/60000 (86%)]\tLoss: 2125770.750000\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 2072687.000000\n",
      "Train Epoch: 1 [52500/60000 (88%)]\tLoss: 2058271.250000\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 2025427.375000\n",
      "Train Epoch: 1 [53500/60000 (89%)]\tLoss: 2042854.000000\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 2043714.375000\n",
      "Train Epoch: 1 [54500/60000 (91%)]\tLoss: 2080163.375000\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 2049634.750000\n",
      "Train Epoch: 1 [55500/60000 (92%)]\tLoss: 1975307.875000\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 2050790.375000\n",
      "Train Epoch: 1 [56500/60000 (94%)]\tLoss: 2012128.375000\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 1969841.125000\n",
      "Train Epoch: 1 [57500/60000 (96%)]\tLoss: 1999757.000000\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 1949740.000000\n",
      "Train Epoch: 1 [58500/60000 (98%)]\tLoss: 1978348.375000\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 1935993.125000\n",
      "Train Epoch: 1 [59500/60000 (99%)]\tLoss: 1943711.375000\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLoss: 1958180.375000\n",
      "Epoch 1 model saved!\n",
      "\n",
      "Test set (10000 samples): Average loss: 41280.7620\n",
      "\n",
      "Train Epoch: 2 [500/60000 (1%)]\tLoss: 1931916.125000\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 1906617.125000\n",
      "Train Epoch: 2 [1500/60000 (2%)]\tLoss: 1902823.125000\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 1900467.500000\n",
      "Train Epoch: 2 [2500/60000 (4%)]\tLoss: 1887569.125000\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 1868546.625000\n",
      "Train Epoch: 2 [3500/60000 (6%)]\tLoss: 1865610.625000\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 1834801.875000\n",
      "Train Epoch: 2 [4500/60000 (8%)]\tLoss: 1867519.875000\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 1834619.250000\n",
      "Train Epoch: 2 [5500/60000 (9%)]\tLoss: 1846837.375000\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 1844607.125000\n",
      "Train Epoch: 2 [6500/60000 (11%)]\tLoss: 1784634.375000\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 1888622.250000\n",
      "Train Epoch: 2 [7500/60000 (12%)]\tLoss: 1844288.750000\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 1809544.500000\n",
      "Train Epoch: 2 [8500/60000 (14%)]\tLoss: 1768885.625000\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 1833118.125000\n",
      "Train Epoch: 2 [9500/60000 (16%)]\tLoss: 1793949.875000\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1775129.375000\n",
      "Train Epoch: 2 [10500/60000 (18%)]\tLoss: 1756769.500000\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 1739292.750000\n",
      "Train Epoch: 2 [11500/60000 (19%)]\tLoss: 1737984.875000\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1737141.500000\n",
      "Train Epoch: 2 [12500/60000 (21%)]\tLoss: 1735744.625000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-637481e833a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# train, test model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresnet_vae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_test_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet_vae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9087e4bbcea5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(log_interval, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # train, test model\n",
    "    X_train, y_train, z_train, mu_train, logvar_train, train_losses = train(log_interval, resnet_vae, device, train_loader, optimizer, epoch)\n",
    "    X_test, y_test, z_test, mu_test, logvar_test, epoch_test_loss = validation(resnet_vae, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "\n",
    "    \n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    \n",
    "    np.save(os.path.join(save_model_path, 'ResNet_VAE_training_loss.npy'), A)\n",
    "    np.save(os.path.join(save_model_path, 'X_MNIST_train_epoch{}.npy'.format(epoch + 1)), X_train) #save last batch\n",
    "    np.save(os.path.join(save_model_path, 'y_MNIST_train_epoch{}.npy'.format(epoch + 1)), y_train)\n",
    "    np.save(os.path.join(save_model_path, 'z_MNIST_train_epoch{}.npy'.format(epoch + 1)), z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
