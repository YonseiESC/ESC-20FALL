{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.0005\n",
    "num_epoch = 1\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Conv2d(1,8,3,padding=1),   # batch x 8 x 28 x 28\n",
    "                        nn.BatchNorm2d(8),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2,2), \n",
    "                        nn.Conv2d(8,16,3,padding=1),  # batch x 16 x 14 x 14\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        nn.Conv2d(16,32,3,padding=1),  # batch x 32 x 7 x 7\n",
    "                        nn.ReLU(),)\n",
    "        \n",
    "        self.fc2_1 = nn.Sequential(\n",
    "                        nn.Linear(32*7*7, 800),\n",
    "                        nn.Linear(800, hidden_size),)\n",
    "        \n",
    "        self.fc2_2 = nn.Sequential(\n",
    "                        nn.Linear(32*7*7, 800),\n",
    "                        nn.Linear(800, hidden_size),)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "                        nn.Linear(hidden_size,800),\n",
    "                        nn.BatchNorm1d(800),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(800,1568),\n",
    "                        nn.ReLU(),)\n",
    "        \n",
    "        self.fc4 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(32,16,3,2,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.ConvTranspose2d(16,8,3,2,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(8),\n",
    "                        nn.ConvTranspose2d(8,1,3,1,1),\n",
    "                        nn.BatchNorm2d(1),)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        #x = x.view(batch_size,-1)\n",
    "        out = self.fc1(x)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = self.relu(out)\n",
    "        mu = self.fc2_1(out)\n",
    "        log_var = self.fc2_2(out)\n",
    "                \n",
    "        return mu,log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        out = self.fc3(z)\n",
    "        out = self.relu(out)\n",
    "        out = out.view(batch_size,32,7,7)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out.view(batch_size,28,28,1)\n",
    "        \n",
    "        return out \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 1, 28, 28))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "  )\n",
       "  (fc2_1): Sequential(\n",
       "    (0): Linear(in_features=1568, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=10, bias=True)\n",
       "  )\n",
       "  (fc2_2): Sequential(\n",
       "    (0): Linear(in_features=1568, out_features=800, bias=True)\n",
       "    (1): Linear(in_features=800, out_features=10, bias=True)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=800, bias=True)\n",
       "    (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=800, out_features=1568, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (fc4): Sequential(\n",
       "    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 1, 28, 28), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Using a target size (torch.Size([100, 1, 28, 28])) that is different to the input size (torch.Size([100, 28, 28, 1])) is deprecated. Please ensure they have the same size.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 645.901836\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 642.946094\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 643.513281\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 642.351523\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 641.682344\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 643.044805\n",
      "====> Epoch: 1 Average loss: 642.9926\n",
      "====> Test set loss: 643.2455\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 644.035586\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 644.240312\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 644.231367\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 644.052383\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 642.243047\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 643.975508\n",
      "====> Epoch: 2 Average loss: 642.9936\n",
      "====> Test set loss: 641.9545\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 643.741406\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 644.107813\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 646.890078\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 643.984453\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 642.418594\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 644.403203\n",
      "====> Epoch: 3 Average loss: 642.9802\n",
      "====> Test set loss: 643.3165\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 643.634727\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 642.745391\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 643.413164\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 641.409180\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 642.322305\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 643.957695\n",
      "====> Epoch: 4 Average loss: 642.9280\n",
      "====> Test set loss: 643.0377\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 643.733906\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 639.758789\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 640.445820\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 644.836406\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 645.515742\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 643.564453\n",
      "====> Epoch: 5 Average loss: 642.8713\n",
      "====> Test set loss: 644.0555\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 642.759063\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 642.433047\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 644.998594\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 641.644805\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 642.296289\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 640.924883\n",
      "====> Epoch: 6 Average loss: 642.9995\n",
      "====> Test set loss: 642.4959\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 641.529063\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 639.776367\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 646.828164\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 645.081289\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 645.730234\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 641.290313\n",
      "====> Epoch: 7 Average loss: 643.0260\n",
      "====> Test set loss: 642.8975\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 645.367109\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 643.258438\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 643.500156\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 640.876797\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 640.067969\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 642.063555\n",
      "====> Epoch: 8 Average loss: 642.8473\n",
      "====> Test set loss: 645.5510\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 643.129727\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 643.838633\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 643.344297\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 642.407969\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 641.408242\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 645.579687\n",
      "====> Epoch: 9 Average loss: 642.9770\n",
      "====> Test set loss: 641.4386\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 642.634922\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 641.018633\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 643.877617\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 640.416484\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 644.523594\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 640.224961\n",
      "====> Epoch: 10 Average loss: 642.8620\n",
      "====> Test set loss: 643.1968\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 643.094609\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 641.185313\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 641.199609\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 644.052070\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 642.789375\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 643.061680\n",
      "====> Epoch: 11 Average loss: 642.9317\n",
      "====> Test set loss: 644.4400\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 641.019727\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 640.029141\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 643.646562\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 644.518203\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 643.944805\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 644.782773\n",
      "====> Epoch: 12 Average loss: 642.9573\n",
      "====> Test set loss: 643.4304\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 642.173320\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 641.758086\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 644.671328\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 638.627266\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 642.517773\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 644.964687\n",
      "====> Epoch: 13 Average loss: 642.8581\n",
      "====> Test set loss: 643.0271\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 642.270352\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 642.479414\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 642.737656\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 642.854141\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 642.779805\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 643.680117\n",
      "====> Epoch: 14 Average loss: 642.8774\n",
      "====> Test set loss: 644.6484\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 643.644141\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 642.443125\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 641.795195\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 643.109102\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 641.244688\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 643.290547\n",
      "====> Epoch: 15 Average loss: 642.9489\n",
      "====> Test set loss: 640.7680\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 642.062578\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 642.325391\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 641.505977\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 643.246836\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 643.199219\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 641.799258\n",
      "====> Epoch: 16 Average loss: 642.9504\n",
      "====> Test set loss: 642.7655\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 643.179180\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 642.037266\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 641.683711\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 642.537695\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 641.603633\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 640.735977\n",
      "====> Epoch: 17 Average loss: 642.8420\n",
      "====> Test set loss: 642.8438\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 642.689219\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 644.722539\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 645.118203\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 641.845117\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 642.198398\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 643.336133\n",
      "====> Epoch: 18 Average loss: 642.8261\n",
      "====> Test set loss: 643.8308\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 645.864219\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 643.710234\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 638.505273\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 642.144023\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 640.699180\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 646.032109\n",
      "====> Epoch: 19 Average loss: 642.8804\n",
      "====> Test set loss: 643.3696\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
